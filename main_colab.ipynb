{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPU-vRAM Usage Estimation for Diffusion Models\n",
        "\n",
        "This notebook estimates peak vRAM usage during inference for Stable Diffusion v1.5.\n",
        "\n",
        "## Steps to Run in Google Colab\n",
        "\n",
        "1. **Enable GPU Runtime**: Go to `Runtime` → `Change runtime type` → Select `T4 GPU`\n",
        "2. **Run cells in order**: Execute each cell sequentially\n",
        "3. **Upload images**: When prompted, either:\n",
        "   - Upload images using the file picker widget, OR\n",
        "   - Use Google Drive mount to access images\n",
        "4. **Enter prompts**: Type your text prompts when asked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run it in Google Colab T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run this first in Colab)\n",
        "!pip install torch torchvision diffusers transformers accelerate hf_xet -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import AutoPipelineForImage2Image\n",
        "from diffusers.utils import make_image_grid, load_image\n",
        "import subprocess\n",
        "import os\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Stable Diffusion pipeline\n",
        "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
        "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
        ")\n",
        "pipeline = pipeline.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get current GPU memory usage via nvidia-smi\n",
        "def get_gpu_memory_via_nvidia():\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n",
        "            capture_output=True, text=True\n",
        "        )\n",
        "        if result.returncode != 0:\n",
        "            return 0\n",
        "        return int(result.stdout.strip()) / 1024\n",
        "    except:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_images_colab():\n",
        "    uploaded = files.upload()\n",
        "    \n",
        "    uploaded_paths = []\n",
        "    for filename in uploaded.keys():\n",
        "        filepath = f\"/content/{filename}\"\n",
        "        with open(filepath, 'wb') as f:\n",
        "            f.write(uploaded[filename])\n",
        "        uploaded_paths.append(filepath)\n",
        "        print(f\"✓ Uploaded: {filename}\")\n",
        "    \n",
        "    return uploaded_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_user_input():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"VRAM Calculator - Image Input\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Select mode:\")\n",
        "    print(\"  [1] Single image\")\n",
        "    print(\"  [2] Multiple images (batch)\")\n",
        "    \n",
        "    while True:\n",
        "        mode = input(\"\\nEnter mode (1 or 2): \").strip()\n",
        "        if mode in ['1', '2']:\n",
        "            break\n",
        "        print(\"Invalid input. Please enter 1 or 2.\")\n",
        "    \n",
        "    img_src = []\n",
        "    \n",
        "    if mode == '1':\n",
        "        # Single image mode\n",
        "        print(\"\\n--- Single Image Mode ---\")\n",
        "        \n",
        "        # Use Colab file upload\n",
        "        uploaded_paths = upload_images_colab()\n",
        "        if uploaded_paths:\n",
        "            image_path = uploaded_paths[0]  # Use first uploaded image\n",
        "        else:\n",
        "            print(\"No image uploaded. Please try again.\")\n",
        "            return get_user_input()\n",
        "        \n",
        "        prompt = input(\"Enter prompt for this image: \").strip()\n",
        "        img_src.append({\"url\": image_path, \"prompt\": prompt})\n",
        "        \n",
        "    else:\n",
        "        # Multiple images mode\n",
        "        print(\"\\n--- Batch Mode (Multiple Images) ---\")\n",
        "        \n",
        "        # Upload all images at once in Colab\n",
        "        print(\"Upload all images you want to process:\")\n",
        "        uploaded_paths = upload_images_colab()\n",
        "        \n",
        "        if not uploaded_paths:\n",
        "            print(\"No images uploaded. Please try again.\")\n",
        "            return get_user_input()\n",
        "            \n",
        "        # Get prompts for each uploaded image\n",
        "        for i, image_path in enumerate(uploaded_paths):\n",
        "            print(f\"\\nImage {i+1}: {os.path.basename(image_path)}\")\n",
        "            prompt = input(f\"Enter prompt for this image: \").strip()\n",
        "            img_src.append({\"url\": image_path, \"prompt\": prompt})\n",
        "            print(f\"✓ Added image {len(img_src)}\")\n",
        "        \n",
        "    print(f\"\\n✓ Total images collected: {len(img_src)}\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "    return img_src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prepare image - get user input\n",
        "img_src = get_user_input()\n",
        "\n",
        "results = list()\n",
        "\n",
        "# Nvidia smi before execution\n",
        "before_execution = get_gpu_memory_via_nvidia()\n",
        "print(\"GPU memory before execution: \", before_execution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting the all the necessary details for the vram calculator pipeline\n",
        "f_parameters = []\n",
        "for i, test_src in enumerate(img_src):\n",
        "    init_image = load_image(test_src.get('url'))\n",
        "\n",
        "    # Getting dimensions\n",
        "    h, w = init_image.size[1], init_image.size[0]\n",
        "\n",
        "    # Getting prompt length or text tokens\n",
        "    prompt = test_src.get('prompt')\n",
        "    tokens = pipeline.tokenizer(prompt, return_tensors=\"pt\")\n",
        "    prompt_length = tokens['input_ids'].shape[1]\n",
        "\n",
        "    f_parameters.append({\n",
        "        'heights': h,\n",
        "        'widths': w,\n",
        "        'prompt_lengths': prompt_length\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process images through the pipeline\n",
        "for _src in img_src:\n",
        "    init_image = load_image(_src.get('url'))\n",
        "    prompt = _src.get('prompt')\n",
        "\n",
        "    # pass prompt and image to pipeline\n",
        "    image = pipeline(prompt, image=init_image, guidance_scale=5.0).images[0]\n",
        "    results.append(make_image_grid([init_image, image], rows=1, cols=2))\n",
        "\n",
        "# Display all processed images\n",
        "for idx, result in enumerate(results):\n",
        "    print(f\"\\nDisplaying result {idx + 1}/{len(results)}\")\n",
        "    display(result)  # Use display() in Colab for better image rendering\n",
        "\n",
        "# Nvidia smi after execution\n",
        "after_execution = get_gpu_memory_via_nvidia()\n",
        "print(\"GPU memory after execution: \", after_execution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stable Diffusion Architecture Understanding\n",
        "\n",
        "- Input = {Prompt, Image}\n",
        "- Image -> VAE encoder -> latent image\n",
        "- Latent image + noise -> noisy latent\n",
        "- Noisy latent -> UNET encoders -> downsampling + extract useful information/features\n",
        "- Prompt -> Text encoder (word2vec) -> Embeddings\n",
        "- Output of UNET encoder + Embeddings -> UNET decoder -> Predicted noise (Upsampling)\n",
        "- Noisy latent - predicted noise = new latent image (by denoising)\n",
        "- New latent image -> VAE decoder -> Completely new image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 1: Base VRAM -> How much VRAM is needed to just load the model?\n",
        "\n",
        "- All the parameters of the model gets loaded into VRAM.\n",
        "- Therefore, calculating the total parameters and weights is required.\n",
        "- Base VRAM = combining total parameters with the weights.\n",
        "- Since, it's fp16 (float 16), each number is going to take 2 bytes. Thus, multiply with 2 to get the bytes used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fp16_bytes = 2\n",
        "\n",
        "# calculating base vram\n",
        "def get_base_vram():\n",
        "    unet_params = sum(component.numel() for component in pipeline.unet.parameters())\n",
        "    vae_params = sum(component.numel() for component in pipeline.vae.parameters())\n",
        "    text_encoder_params = sum(component.numel() for component in pipeline.text_encoder.parameters())\n",
        "\n",
        "    total_params = unet_params + vae_params + text_encoder_params\n",
        "\n",
        "    weights = total_params * fp16_bytes\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 2: Activations (variable memory) -> How much VRAM will be required based on the input?\n",
        "\n",
        "1. VAE encoder -> depends on image (height and width)\n",
        "2. UNET -> depends on image and prompt (height, width, prompt_length)\n",
        "3. VAE decoder -> depends on image (height and width)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phase 2.1. VAE Encoder\n",
        "\n",
        "1. Amount of memory needed to store in the GPU (VRAM).\n",
        "   - During the processing, the VAE stores up some temporary tensors.\n",
        "   - Temporary tensors does take up memory which is estimated to be 3 to 4 times the input memory.\n",
        "\n",
        "2. Memory needed for compressed image or latent image. Eg:- (512 x 512 x 3) image -> (64 x 64 x 4).\n",
        "   - Compression = 8x (512 / 64 = 8), total 64x  \n",
        "   - And channel is changed from 3 to 4 for storing for essential information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def latent_values(height, width):\n",
        "    latent_height = height // 8\n",
        "    latent_width = width // 8\n",
        "\n",
        "    latent_memory = latent_height * latent_width * 4 * fp16_bytes\n",
        "    return latent_height, latent_width, latent_memory\n",
        "\n",
        "\n",
        "def vae_encoder_memory(height, width):\n",
        "    input_memory = height * width * 3 * fp16_bytes\n",
        "    temp_memory = input_memory * 3\n",
        "    _, _, latent_memory = latent_values(height, width)\n",
        "\n",
        "    total_memory = input_memory + temp_memory + latent_memory\n",
        "    return total_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phase 2.2. UNET\n",
        "\n",
        "For the image:\n",
        "- Input will be latent values from vae encoder.\n",
        "- Initial layers take up lot of memory.\n",
        "- As we go down memory reduces and channel goes up. Reason being information gets compressed.\n",
        "- So, I decided to take average channels of the layers (Calculating each one is too complex.)\n",
        "\n",
        "A typical layer structure (got from hugging face):\n",
        "- 320\n",
        "- 640\n",
        "- 1280\n",
        "\n",
        "Average = (320 + 640 + 1280) / 3 = 746.667 => 745\n",
        "- Similar to VAE, unet also uses temporary tensor memory, assuming it to be 3x.\n",
        "\n",
        "For the prompt:\n",
        "- prompt length -> text tokens\n",
        "- attention matrix is calculated.\n",
        "- Attention matrix - mapping words to the image regions. (Letting the LLM know what part of the image to be influenced by which word)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def unet_memory(height, width, prompt_length):\n",
        "    latent_h, latent_w, _ = latent_values(height, width)\n",
        "    channel_activations = latent_h * latent_w * 745 * fp16_bytes\n",
        "    channel_memory = channel_activations * 3\n",
        "\n",
        "    attention_matrix = latent_h * latent_w * prompt_length * fp16_bytes\n",
        "\n",
        "    total_memory = channel_memory + attention_matrix\n",
        "    return total_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phase 2.3 VAE Decoder\n",
        "\n",
        "- Input is the denoised image from unet, which is basically the latent image.\n",
        "- However, only the image will be different while dimensions will be same as the latent image which was fed to UNET.\n",
        "- Then, it produces a new image which will have same dimensions as the original image.\n",
        "- And during the creation of new image, it generates temporary tensors assuming it to be 3x memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vae_decoder_memory(height, width):\n",
        "    _, _, latent_memory = latent_values(height, width)  # input\n",
        "    output_memory = height * width * 3 * fp16_bytes  # output image\n",
        "    temp_memory = output_memory * 3\n",
        "\n",
        "    total_memory = latent_memory + output_memory + temp_memory\n",
        "    return total_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VRAM Calculation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(h: int, w: int, prompt_length: int, **kwargs):\n",
        "    \"\"\"\n",
        "    :param h: height of input image in pixels\n",
        "    :param w: width of input image in pixels\n",
        "    :param prompt_length: length of input prompt in number tokens generated after tokenizing the input-prompt.\n",
        "    :param kwargs: any additional factors needed for this computation (this is for your use)\n",
        "    \"\"\"\n",
        "    base_mem = get_base_vram()\n",
        "    encod_mem = vae_encoder_memory(h, w)\n",
        "    unet_mem = unet_memory(h, w, prompt_length)\n",
        "    decod_mem = vae_decoder_memory(h, w)\n",
        "\n",
        "    peak_mem = max(encod_mem, unet_mem, decod_mem)\n",
        "\n",
        "    # Assuming a 20% extra memory.\n",
        "    # So, total_memory_percentage = 100 + 20 = 120%\n",
        "    # overhead_memory_value = 120 / 100 = 1.2\n",
        "    overhead_fact = 1.2\n",
        "\n",
        "    total_mem = (base_mem + peak_mem) * overhead_fact\n",
        "    total_mem_gb = total_mem / (1024**3)\n",
        "\n",
        "    return total_mem, total_mem_gb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_mem_req = 0\n",
        "\n",
        "for params in f_parameters:\n",
        "    _, each_mem_req = f(params['heights'], params['widths'], params['prompt_lengths'])\n",
        "    total_mem_req = total_mem_req + each_mem_req\n",
        "\n",
        "print(\"My function prediction: \", total_mem_req)\n",
        "print(\"Nvidia SMI calculations: \", after_execution)\n",
        "print(\"Difference in error: \", total_mem_req - after_execution)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
